{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.app import Redbox\n",
    "from redbox.models.settings import Settings, get_settings\n",
    "from redbox.models.chain import RedboxQuery, RedboxState, AISettings, ChatLLMBackend\n",
    "from langfuse.callback import CallbackHandler\n",
    "from uuid import uuid4\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "import langchain\n",
    "\n",
    "from uuid import UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class AWSBedrock(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = BedrockChat(\n",
    "    credentials_profile_name=\"default\", # e.g. \"default\"\n",
    "    region_name=\"eu-west-2\", # e.g. \"us-east-1\"\n",
    "    endpoint_url=f\"https://bedrock-runtime.eu-west-2.amazonaws.com\", # e.g. \"https://bedrock-runtime.us-east-1.amazonaws.com\"\n",
    "    model_id=\"mistral.mixtral-8x7b-instruct-v0:1\", # e.g. \"anthropic.claude-v2\"\n",
    "    model_kwargs={\"temperature\": 0.4},\n",
    ")\n",
    "\n",
    "aws_bedrock = AWSBedrock(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"richie.bird@digital.trade.gov.uk/associated-british-ports-financial-statements-2023.pdf\"]\n",
    "user_prompt_summarise = [\"summarise the document\"]\n",
    "user_prompt_search = [\"what are the expansion plans of ABP?\"]\n",
    "# user_prompt_search = [\"@search what are the expansion plans of ABP?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(user_uuid, prompts, documents, ai_setting):\n",
    "    q = RedboxQuery(\n",
    "        question=f\"{prompts[-1]}\",\n",
    "        s3_keys=documents,\n",
    "        user_uuid=user_uuid,\n",
    "        chat_history=prompts[:-1],\n",
    "        ai_settings=ai_setting,\n",
    "        permitted_s3_keys=documents,\n",
    "    )\n",
    "\n",
    "    return RedboxState(\n",
    "        request=q,\n",
    ")\n",
    "\n",
    "env = get_settings()\n",
    "env = env.model_copy(update={\"elastic_root_index\": 'redbox-data-integration'})\n",
    "env = env.model_copy(update={\"elastic_chunk_alias\": 'redbox-data-integration-chunk-current'})\n",
    "\n",
    "ai_setting = AISettings(chat_backend=ChatLLMBackend(name=\"anthropic.claude-3-sonnet-20240229-v1:0\", provider=\"bedrock\"))\n",
    "app = Redbox(debug=False, env=env)\n",
    "\n",
    "x = get_state(uuid4(), prompts = user_prompt_summarise, documents = documents, ai_setting = ai_setting)\n",
    "final_state_summarise = await app.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state_summarise.messages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state_summarise.messages[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"/Users/richiebird/Downloads/associated-british-ports-financial-statements-2023.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "content = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "    content.append(page.page_content)\n",
    "\n",
    "content_subset = \" \".join(content[0:5]) # cant use all the pages, context window not large enough and langchain errors/hangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=user_prompt_summarise[0],\n",
    "    actual_output=final_state_summarise.messages[1].content,\n",
    "    context=content[0:5],\n",
    ")\n",
    "metric = HallucinationMetric(threshold=0.3, model=aws_bedrock)\n",
    "# To run metric as a standalone\n",
    "# metric.measure(test_case)\n",
    "# print(metric.score, metric.reason)\n",
    "\n",
    "hallucination = evaluate(test_cases=[test_case], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import SummarizationMetric\n",
    "\n",
    "test_case = LLMTestCase(input=content_subset, actual_output=final_state_summarise.messages[1].content)\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=aws_bedrock,\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# To run metric as a standalone\n",
    "# metric.measure(test_case)\n",
    "# print(metric.score, metric.reason)\n",
    "\n",
    "evaluate(test_cases=[test_case], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating RAG and our retrieved chunks\n",
    "\n",
    "from opensearchpy import OpenSearch,  RequestsHttpConnection, client\n",
    "#\"http://admin:admin@opensearch:9200\"\n",
    "client = OpenSearch(\n",
    "            hosts=[{\"host\": \"localhost\", \"port\": \"9200\"}],\n",
    "            http_auth=(\"admin\", \"admin\"),\n",
    "            use_ssl=False, #to run locally, changed from True to False\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            retry_on_timeout=True\n",
    "        )\n",
    "\n",
    "query = {\n",
    "    \"size\": 1000,\n",
    "    \"track_total_hits\": True,\n",
    "    \"query\" : {\n",
    "        \"match_all\" : {}\n",
    "    }\n",
    "}\n",
    "\n",
    "#redbox-data-integration-chunk-current\n",
    "\n",
    "response = client.search(index='redbox-data-integration-chunk-current', body=query)\n",
    "print(response)\n",
    "\n",
    "client.indices.get_mapping(index='redbox-data-integration-chunk-current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class AISettings(BaseModel):\n",
    "    \"\"\"Prompts and other AI settings\"\"\"\n",
    "\n",
    "    # LLM settings\n",
    "    context_window_size: int = 128_000\n",
    "    llm_max_tokens: int = 1024\n",
    "\n",
    "    # Prompts and LangGraph settings\n",
    "    max_document_tokens: int = 1_000_000\n",
    "    self_route_enabled: bool = False\n",
    "    map_max_concurrency: int = 128\n",
    "    stuff_chunk_context_ratio: float = 0.75\n",
    "    recursion_limit: int = 50\n",
    "\n",
    "    # Elasticsearch RAG and boost values\n",
    "    rag_k: int = 1000\n",
    "    rag_num_candidates: int = 1000\n",
    "    rag_gauss_scale_size: int = 3\n",
    "    rag_gauss_scale_decay: float = 0.5\n",
    "    rag_gauss_scale_min: float = 1.1\n",
    "    rag_gauss_scale_max: float = 2.0\n",
    "    elbow_filter_enabled: bool = False\n",
    "    match_boost: float = 1.0\n",
    "    match_name_boost: float = 2.0\n",
    "    match_description_boost: float = 0.5\n",
    "    match_keywords_boost: float = 0.5\n",
    "    knn_boost: float = 2.0\n",
    "    similarity_threshold: float = 0.7\n",
    "\n",
    "    # this is also the azure_openai_model\n",
    "    #chat_backend: ChatLLMBackend = ChatLLMBackend()\n",
    "\n",
    "    # settings for tool call\n",
    "    tool_govuk_retrieved_results: int = 100\n",
    "    tool_govuk_returned_results: int = 5\n",
    "    \n",
    "ai_settings = AISettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(region_name='eu-west-2', model_id=\"amazon.titan-embed-text-v2:0\")\n",
    "query_vector = embedding_model.embed_query(user_prompt_search[0])\n",
    "\n",
    "query_filter = [{\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\"terms\": {\n",
    "                    \"metadata.file_name.keyword\": [\n",
    "                        \"richie.bird@digital.trade.gov.uk/associated-british-ports-financial-statements-2023.pdf\"\n",
    "                        ]}},\n",
    "                {\"terms\": {\"metadata.uri.keyword\": [\n",
    "                    \"richie.bird@digital.trade.gov.uk/associated-british-ports-financial-statements-2023.pdf\"\n",
    "                    ]}}\n",
    "            ]\n",
    "        }\n",
    "    }, {\"term\": {\"metadata.chunk_resolution.keyword\": \"normal\"}}]\n",
    "\n",
    "knn_final_query = {\n",
    "        \"size\": ai_settings.rag_k,\n",
    "                   \"min_score\": 0.6,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"knn\": {\n",
    "                            \"vector_field\": {\n",
    "                            \"vector\": query_vector,\n",
    "                            \"k\": ai_settings.rag_num_candidates,\n",
    "                            # \"boost\": ai_settings.knn_boost,\n",
    "                            \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                ],\n",
    "                \"filter\": query_filter,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "response_knn = client.search(index='redbox-data-integration-chunk-current', body=knn_final_query)\n",
    "response_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_texts = []\n",
    "for i, j in enumerate(response_knn['hits']['hits']):\n",
    "    page = response_knn['hits']['hits'][i]['_source']['metadata']['page_number']\n",
    "    score = response_knn['hits']['hits'][i]['_score']\n",
    "    retrieved_texts.append(response_knn['hits']['hits'][i]['_source']['text'])\n",
    "    print(f'{i}, {page}, {score}')\n",
    "    # chunk retrieval limit is 30\n",
    "    if i == 29: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_state(uuid4(), prompts = user_prompt_search, documents = documents, ai_setting = ai_setting)\n",
    "final_state_search = await app.run(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
